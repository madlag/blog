<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Sparse Neural Networks (2/N): GPU Performance.</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Sparse Neural Networks (2/N): GPU Performance.</h1>
</header>
<section data-field="subtitle" class="p-summary">
NVIDIA Ampere A100 introduces fine-grained structured sparsity
</section>
<section data-field="body" class="e-content">
<section name="62d4" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="854d" id="854d" class="graf graf--h3 graf--leading graf--title">Sparse Neural Networks (2/N): Understanding GPU Performance.</h3><p name="2557" id="2557" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">NVIDIA Ampere A100 introduces fine-grained structured sparsity</strong></p><p name="8708" id="8708" class="graf graf--p graf-after--p">Welcome back for this series on Sparse Neural Networks. In case you have not read our first introductory episode, <a href="https://link.medium.com/In4bINyeO3" data-href="https://link.medium.com/In4bINyeO3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here it is</a>.</p><p name="edbd" id="edbd" class="graf graf--p graf-after--p">I told you last time that<strong class="markup--strong markup--p-strong"> sparsity would a major topic in 2020</strong>, and it looks like it’s getting indeed some steam: <strong class="markup--strong markup--p-strong">Nvidia</strong> is announcing with the <strong class="markup--strong markup--p-strong">Ampere</strong> GPU generation that<strong class="markup--strong markup--p-strong"> sparsity is directly baked into their GPU design.</strong></p><p name="679a" id="679a" class="graf graf--p graf-after--p">It’s quite a bold move: if you consider the time it takes to design and produce a new GPU line, they made this decision at least 2 years ago, and you need some vista to understand that it would be an important trend 2 years later.</p><figure name="b684" id="b684" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 337px; max-height: 331px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 98.2%;"></div><img class="graf-image" data-image-id="1*M1ky4jno_MmGg5eR3tgGyg.png" data-width="337" data-height="331" alt="André Ampère" src="https://cdn-images-1.medium.com/max/600/1*M1ky4jno_MmGg5eR3tgGyg.png"></div><figcaption class="imageCaption">André Ampère, 1825 (from Wikipedia)</figcaption></figure><p name="590a" id="590a" class="graf graf--p graf-after--figure">So that’s the perfect pretext to make a large digression on <strong class="markup--strong markup--p-strong">GPU architectures</strong> and why knowing better about them may matter for your daily Machine Learning jobs.</p><p name="27f1" id="27f1" class="graf graf--p graf-after--p">To be honest, this will more matter to you if you are working on some low-level code.</p><p name="485b" id="485b" class="graf graf--p graf-after--p">If you are using PyTorch or other libraries, and you are just using the extremely good tools it provides, you are probably fine.</p><p name="cc5b" id="cc5b" class="graf graf--p graf-after--p">But<strong class="markup--strong markup--p-strong"> </strong><a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/" data-href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">leaky abstractions</strong></a><strong class="markup--strong markup--p-strong"> come back at you </strong>faster than you’d think<strong class="markup--strong markup--p-strong">.</strong> Your model got a bit heavier? Want to train faster? OK, let’s use a DataParallel PyTorch node, and we’ll be fine on 8 GPUs. But wait, why my GPU usage is down the gutter? And on 8 GPUs it’s only 3 times as fast as on a single one?</p><p name="fc80" id="fc80" class="graf graf--p graf-after--p">It especially matters to me, as I have been telling you last time that the <strong class="markup--strong markup--p-strong">performance of sparse matrices operations was not satisfactory</strong>. Today we’ll see why it can be hard to get good performance on GPUs, how it depends on your data structure and algorithms, and how you can overcome it, or at some times at least mitigate some issues.</p><p name="e654" id="e654" class="graf graf--p graf-after--p">And of course, all this is a good pretext to read about some mind-blowing GFlops numbers and killer optimizations, nothing to sneeze at…</p><h3 name="186b" id="186b" class="graf graf--h3 graf-after--p">Some physics</h3><p name="dfd2" id="dfd2" class="graf graf--p graf-after--h3">You may wonder why your PC/Mac is not significantly faster than a few years ago. That’s because most of the apps you are using are mostly sequential: they are doing only one thing at a time, or almost, and sequential performance has been stagnating for some years.</p><p name="cefe" id="cefe" class="graf graf--p graf-after--p">That’s because sequential performance is mostly limited by <strong class="markup--strong markup--p-strong">operating</strong> <strong class="markup--strong markup--p-strong">frequency</strong>, which is itself limited by:</p><ul class="postList"><li name="b644" id="b644" class="graf graf--li graf-after--p">the<strong class="markup--strong markup--li-strong"> size of the finest details</strong> that are drawn on the silicon, something that is getting harder and harder to improve,</li><li name="8ee4" id="8ee4" class="graf graf--li graf-after--li">the amount of <strong class="markup--strong markup--li-strong">heat</strong> that is created by the chips, a function of voltage and frequency. First, a <strong class="markup--strong markup--li-strong">transistor</strong> emits heat when <strong class="markup--strong markup--li-strong">changing state</strong>, so proportionally to frequency. Second, <strong class="markup--strong markup--li-strong">the higher the frequency, the higher the voltage</strong> you need. So in the end <strong class="markup--strong markup--li-strong">emitted heat is more than linear in the frequency</strong>, not something ideal.</li></ul><figure name="3268" id="3268" class="graf graf--figure graf--layoutOutsetLeft graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 341px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 64.9%;"></div><img class="graf-image" data-image-id="1*0b-KUkO2e22UPpPKrRi9PQ.png" data-width="2058" data-height="1336" data-is-featured="true" src="https://cdn-images-1.medium.com/max/600/1*0b-KUkO2e22UPpPKrRi9PQ.png"></div><figcaption class="imageCaption">From <a href="https://youtu.be/Knd-U-avG0c?t=109" data-href="https://youtu.be/Knd-U-avG0c?t=109" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://youtu.be/Knd-U-avG0c</a></figcaption></figure><p name="0585" id="0585" class="graf graf--p graf-after--figure">So if you could efficiently and cheaply remove heat from the chips, you could get higher frequencies, but only marginally, and it gets quickly impractical (water-cooling, you know, is <em class="markup--em markup--p-em">cool</em>, but <a href="https://www.avadirect.com/blog/leaking-liquid-cooler-whats-next/" data-href="https://www.avadirect.com/blog/leaking-liquid-cooler-whats-next/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">not when it leaks</a>…).</p><p name="3e19" id="3e19" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The recent ARM takeover is not an accident</strong>. When you work for years on <strong class="markup--strong markup--p-strong">low consumption and so low heat producing chips,</strong> when everybody hits the “heat wall”, you are in a <strong class="markup--strong markup--p-strong">good position to</strong><a href="https://9to5mac.com/2018/11/01/geekbench-ipad-pro-performance/" data-href="https://9to5mac.com/2018/11/01/geekbench-ipad-pro-performance/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong"> push performance higher</strong></a>, even if computers migrating to your pocket was the opportunity that made the difference.</p><h3 name="5e10" id="5e10" class="graf graf--h3 graf-after--p">Chip design</h3><p name="d152" id="d152" class="graf graf--p graf-after--h3">So people invented tricks to make use of the same amount of cycles to do more, to do almost any instruction in one single cycle, to forecast what’s the next instruction etc. Very different architectures to tackle the same issues were used (<a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/risccisc/" data-href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/risccisc/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">RISC, CISC</a>). But the returns are diminishing, as always.</p><p name="a51d" id="a51d" class="graf graf--p graf-after--p">So what can you do to feed the hungry “Moore’s Law Beast”, and the marketing guys who keep asking why the numbers are flattening?</p><p name="2d01" id="2d01" class="graf graf--p graf-after--p">You look for problems that need to do the same kind of task a billion times, and each task does not need the result of another task, so all tasks can be computed at the same time. (the technical slang for this is “<a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel" data-href="https://en.wikipedia.org/wiki/Embarrassingly_parallel" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Embarrassingly Parallel</strong></a>” …).</p><p name="7eb1" id="7eb1" class="graf graf--p graf-after--p">Fortunately, there are a lot of them. <strong class="markup--strong markup--p-strong">Linear Algebra</strong>, for example, is highly parallel by nature, and <strong class="markup--strong markup--p-strong">machine learning</strong> is using it a lot, like lots of <strong class="markup--strong markup--p-strong">physics</strong> <strong class="markup--strong markup--p-strong">simulation</strong>, <strong class="markup--strong markup--p-strong">computer</strong> <strong class="markup--strong markup--p-strong">graphics</strong>, and so on.</p><p name="2444" id="2444" class="graf graf--p graf-after--p">So instead of increasingly complex single cores processors, we see much simpler (and smaller on silicon) cores but grouped by the hundreds or thousands. This way you are guaranteed that the ratio computation/silicon area is getting through the roof.</p><p name="5335" id="5335" class="graf graf--p graf-after--p">Great. That’s a simple idea. But of course, reality is more complex than that.</p><h3 name="19d9" id="19d9" class="graf graf--h3 graf-after--p">Bottlenecks</h3><p name="9962" id="9962" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">If you have a lot of computing power available, you have to feed it with data. </strong>Memory is getting faster with time, but it’s harder than just duplicating cores. Because<strong class="markup--strong markup--p-strong"> memory buses are basically 1D, and compute cores are 2D</strong>.</p><figure name="abb1" id="abb1" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 336px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 63.9%;"></div><img class="graf-image" data-image-id="1*Fruof2lsq_v7hdrXZUhRjg.png" data-width="3964" data-height="2534" src="https://cdn-images-1.medium.com/max/600/1*Fruof2lsq_v7hdrXZUhRjg.png"></div><figcaption class="imageCaption">From <a href="https://unsplash.com/photos/VEVfbQtyB8s" data-href="https://unsplash.com/photos/VEVfbQtyB8s" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://unsplash.com/photos/VEVfbQtyB8s</a></figcaption></figure><p name="ad4f" id="ad4f" class="graf graf--p graf-after--figure">You can think about it as a <strong class="markup--strong markup--p-strong">city</strong> (the computing cores), and the suburban workers coming each morning in the city (the data). <strong class="markup--strong markup--p-strong">The city is 2D, the highways are 1D,</strong> and of course, you get some heavy traffic jams. So you add some new <strong class="markup--strong markup--p-strong">lanes</strong> on the <strong class="markup--strong markup--p-strong">highways</strong> (the width of the memory bus), but it’s always the <strong class="markup--strong markup--p-strong">bottleneck</strong></p><p name="45e0" id="45e0" class="graf graf--p graf-after--p">If you want to maximize the highway utility, you would have to use all day long, encouraging people to come to and leave from work earlier or later.</p><p name="8357" id="8357" class="graf graf--p graf-after--p">That’s the same thing for the memory bus: <strong class="markup--strong markup--p-strong">you have to make sure that you are balancing computation and memory transfers so you don’t waste time waiting without using the memory bus or the compute cores</strong>. That’s why it’s hard to reach peak performance for every task.</p><p name="7a49" id="7a49" class="graf graf--p graf-after--p">Some tasks even prefer to compute twice the same thing instead of transferring some data: <strong class="markup--strong markup--p-strong">compute is plentiful and memory bandwidth is scarce </strong>(and the gap is growing each year). In graphics, procedural texturing is used more and more for this exact reason: textures need bandwidth, and so if you can generate the same result with few memory transfers but some additional compute, it’s a win.</p><h3 name="4d0b" id="4d0b" class="graf graf--h3 graf-after--p">GPU Architecture principles</h3><p name="2747" id="2747" class="graf graf--p graf-after--h3">A lot of the complexities of GPU architectures exist to overcome those bottlenecks.</p><h4 name="93dc" id="93dc" class="graf graf--h4 graf-after--p">Hierarchy</h4><p name="5254" id="5254" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">You don’t get the 1000s of cores in a GPU in a single bag: they are grouped at multiple levels. </strong>We’ll take the example of the new Ampere A100. Numbers change according to the generation, but the general principles are slowly evolving. (Numbers below come mostly from the <a href="https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/" data-href="https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Nvidia blog</a>)</p><figure name="fcc3" id="fcc3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 924px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 132%;"></div><img class="graf-image" data-image-id="1*i60_7GqIKxjgunaXe7CiCw.png" data-width="1212" data-height="1600" src="https://cdn-images-1.medium.com/max/800/1*i60_7GqIKxjgunaXe7CiCw.png"></div><figcaption class="imageCaption"><em class="markup--em markup--figure-em">The GA100 streaming multiprocessor (SM)</em></figcaption></figure><p name="d005" id="d005" class="graf graf--p graf-after--figure">At the lower level you have a Streaming Processor (SP). He is part of a group of 16 SP which computes the same sequence of instructions at the same time.</p><p name="ee0c" id="ee0c" class="graf graf--p graf-after--p">(To be more precise, you have 16 FP32 cores, 8 FP64 cores, 16 INT32 cores, 1 Tensor Core, and 1 texture unit per group. More on tensor cores later)</p><p name="f262" id="f262" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The first constraint is the following: the 16 SP in the group cannot diverge from a single sequence instruction. This is called SIMD: Same Instruction, Multiple Data.</strong> That’s not exactly true, the instructions can contain “if“ statement, but if different branches are taken, some compute will be lost because every processor will have to execute both branches, and throw the results that are not useful for its own work.</p><p name="560a" id="560a" class="graf graf--p graf-after--p">4 groups of 16 SPs form a Streaming Multiprocessor (SM). Each group executes the same kernel (=function), but not in a strictly synchronized way. Still, you’ll have at least 64 cores working on the same task, or you’ll lose some computing capacity.</p><p name="5d6a" id="5d6a" class="graf graf--p graf-after--p">Then, you group 2 SMs to form a “Texture Processing Cluster” (TPC), and you group 8 TPCs to form a GPC (GPU Processing Cluster). 8GPCs and you have an A100 GPU. Pfew!</p><p name="50c1" id="50c1" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">To sum it up, there are 128 SMs in an A100, so 8192 FP32 cores, but as you can see, we are far from getting a flat set of “8192” cores! </strong>(those are maximum numbers, first processors won’t have the full set of cores)<strong class="markup--strong markup--p-strong">.</strong></p><p name="5c5d" id="5c5d" class="graf graf--p graf-after--p">If you compare the A100 structure with the Volta V100, these structural numbers are almost the same, except for the PCs, and so for the grand total of course. The innards of the cores of course have changed too, but it looks like that the communication structure of the V100 was considered quite good for the kind of job it’s usually given. The Tensor Cores seems to be the area where the most innovation is taking place (more on this later).</p><p name="4bff" id="4bff" class="graf graf--p graf-after--p">You can see in the comparison below that all those numbers varied significantly with time, in search of the best performance :</p><figure name="fd12" id="fd12" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 700px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 100%;"></div><img class="graf-image" data-image-id="1*MmPCD_ypvtHDhwAGUZhqHQ.png" data-width="1098" data-height="1098" src="https://cdn-images-1.medium.com/max/800/1*MmPCD_ypvtHDhwAGUZhqHQ.png"></div></figure><h4 name="a100" id="a100" class="graf graf--h4 graf-after--figure">Why so many levels? Performance</h4><p name="e6e4" id="e6e4" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">The main reason is of course to improve real-life performance. And in real life, you don’t have a single task to be done.</strong></p><p name="616f" id="616f" class="graf graf--p graf-after--p">First, there may be several processes using your GPU at the same time on your machine. Not sure if it’s a good idea to get some good performance, but it’s of course something very usual.</p><p name="5993" id="5993" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">In the new Ampere GPU, you can even partition your GPU to server multiple Virtual Machines with strong guarantees on your data security: the new feature is called “Multi-Instance GPU”.</strong></p><p name="3e96" id="3e96" class="graf graf--p graf-after--p">In a single process, if your network contains <strong class="markup--strong markup--p-strong">several layers</strong>, some linear, some non-linear, some embedding, each one will use one or <strong class="markup--strong markup--p-strong">several kernels</strong> to do its job.</p><p name="b768" id="b768" class="graf graf--p graf-after--p">You may think that they are executed one after the other. It’s true to some extent, but in order to<strong class="markup--strong markup--p-strong"> keep your GPU busy</strong>, your <strong class="markup--strong markup--p-strong">CPU</strong> is sending a <strong class="markup--strong markup--p-strong">stream of tasks to be done</strong>, not a task after the other, and the GPU will do them without the CPU waiting for each one to complete.</p><p name="fe19" id="fe19" class="graf graf--p graf-after--p">The CPU will basically wait after a full batch has been processed, after the forward and backward pass, because he has to update the full model before starting a new batch.</p><p name="cfe6" id="cfe6" class="graf graf--p graf-after--p">There are several reasons to have this “stream of task” model:</p><ul class="postList"><li name="0487" id="0487" class="graf graf--li graf-after--p">The first reason is that starting a task takes some time, so the GPU can prepare the next task before the previous is started: changing the active kernel on some part of the GPU takes some time, <strong class="markup--strong markup--li-strong">pipelining</strong> saves time.</li><li name="1b41" id="1b41" class="graf graf--li graf-after--li">Second, in the task stream, some tasks are not dependent on each other, so both can be executed in parallel in the GPU, so more work to be done, so less chance some part of the GPU is idling.</li></ul><p name="a7d9" id="a7d9" class="graf graf--p graf-after--li">Some <strong class="markup--strong markup--p-strong">networks</strong> are <strong class="markup--strong markup--p-strong">very, very parallel </strong>to compute, like <strong class="markup--strong markup--p-strong">Transformers</strong>, and so their efficiency is very good:</p><ul class="postList"><li name="20c6" id="20c6" class="graf graf--li graf-after--p">there are only a <strong class="markup--strong markup--li-strong">few different layers</strong>, so <strong class="markup--strong markup--li-strong">few kernel changes</strong> and a lot of work for each kernel</li><li name="cd2b" id="cd2b" class="graf graf--li graf-after--li">there are only <strong class="markup--strong markup--li-strong">loose dependencies between computations</strong> (eg for each token), so the GPU has a lot of degrees of freedom when scheduling the different parts of the computation: if a kernel is waiting for some data, maybe another one can compute its result because it already has its own data available.</li></ul><h4 name="d636" id="d636" class="graf graf--h4 graf-after--li">Why so many levels? Economics</h4><p name="14d4" id="14d4" class="graf graf--p graf-after--h4">Another reason is that it’s hard to get zero-defect silicon at this level of detail.</p><p name="715d" id="715d" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Ampere GPUs</strong> contain <strong class="markup--strong markup--p-strong">54 billion transistors</strong>. Any defective transistor, and you may have to throw the GPU to the bin. <strong class="markup--strong markup--p-strong">The fraction of chips that pass the test is called the <em class="markup--em markup--p-em">yield</em>.</strong> Those chips are huge, and silicon real estate costs a lot, so each failed chip is a big loss, just for a small defect on a single transistor somewhere in the silicon.</p><p name="193d" id="193d" class="graf graf--p graf-after--p">So instead of throwing the chip to the bin, <strong class="markup--strong markup--p-strong">you test some sub-parts of the chip, and you just disable the failing sub-parts</strong>. That means, for example, disabling a GPC (remember, there are 7 of them in a A100, instead of a theoretical 8). And you sell it in a lower-end card, <strong class="markup--strong markup--p-strong">with reduced specs</strong>. This process is called <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">binning</em></strong>. If you are really good, and your chips are all perfect, you may even disable perfectly working parts of your chip, to segment your offer (and back in time, some users were able to re-enable those disabled parts of silicon to get the bang without the buck…)</p><h3 name="5c34" id="5c34" class="graf graf--h3 graf-after--p">Developing for GPUs</h3><p name="81c0" id="81c0" class="graf graf--p graf-after--h3">So what are the consequences of the GPU architecture choices on development?</p><h4 name="924d" id="924d" class="graf graf--h4 graf-after--p">Kernels</h4><p name="4f7f" id="4f7f" class="graf graf--p graf-after--h4">First, <strong class="markup--strong markup--p-strong">you have to write some kernels,</strong> using the primitives you get. It’s a quite specific exercise, as you have to manually manage caches, registers, the synchronization of the different cores, etc. For simple stuff like matrix products, or activation layers, it’s quite straightforward, as they are completely parallel by nature.</p><p name="3af4" id="3af4" class="graf graf--p graf-after--p">But for some algorithms, like sorting, it can be a lot trickier to have something efficient, because you will have some issues using all the cores all the time.</p><h4 name="4d99" id="4d99" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Grids and performance</strong></h4><p name="9476" id="9476" class="graf graf--p graf-after--h4">That’s because the kernel is only a small part of the problem, the other is the way you distribute the work among cores. And the performance gains are often made more on the distribution than on an optimal kernel.</p><p name="9eea" id="9eea" class="graf graf--p graf-after--p">The way you distribute the work is usually done by<strong class="markup--strong markup--p-strong"> partitioning your job into a 2D or 3D grid, </strong>then mapping each point of the grid to a thread, and finally mapping those threads to physical cores. Those dimensions will correspond for example to the dimensions of the output of a layer, plus the batch dimension.</p><p name="1fc6" id="1fc6" class="graf graf--p graf-after--p">As you have seen, in a GPU you get thousands of cores to work with, but with a really complex multi-layered structure. And this structure change according to the generation and model of the GPU. <strong class="markup--strong markup--p-strong">So it’s hard to find the right way to choose those mappings.</strong> You often have to make some benchmarks to find the right way to do a computation with given dimensions on a specific GPU, and that information will be used in the future to choose the best strategy at runtime.</p><h4 name="2b6d" id="2b6d" class="graf graf--h4 graf-after--p">Memory</h4><p name="b597" id="b597" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">But the main and the most difficult hurdle a developer face while developing for GPU’s architecture is managing memory.</strong> And specifically memory transfers. The available memory bandwidth is huge, but the computing power is even larger. And just as you did not get a flat space of computing cores, you don’t get completely random access to the memory for free.</p><p name="b683" id="b683" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">If you want to access a float number stored in the main memory from a GPU core, you will wait literally for <em class="markup--em markup--p-em">ages </em>compared to the time it takes to compute a sum or a multiply.</strong> So you need to be able to start hundreds of computations at once, and when the data is finally available, you resume your kernel, you execute a few local operations, until you need some more data from the main memory.</p><p name="7ba2" id="7ba2" class="graf graf--p graf-after--p">Some special ops like “prefetch” exist, to declare that you will need some data in a few instructions, and the role of the compiler is to reorder the instructions so you keep the memory controllers busy while keeping the core busy too. And at runtime, a large part of the GPU silicon is devoted to handling all those threads that are “in flight” and their current memory requests.</p><p name="cf2b" id="cf2b" class="graf graf--p graf-after--p">But there are some low-level constraints that may cost you a lot. Just like the base computation unit is 16 cores doing the same job,<strong class="markup--strong markup--p-strong"> you really get peak memory performance if you load memory by quite large contiguous blocks, </strong>for example, 16 floats = 64 bytes, by a group of threads (called warp in CUDA lingo). This is called <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">coalesced</em></strong> access. This is another reason, and often the main one, why choosing the right grid to dispatch your task on is important.</p><p name="deb3" id="deb3" class="graf graf--p graf-after--p">So now, let’s unroll back to our initial issue if you still remember (I would forgive you, I can barely): <strong class="markup--strong markup--p-strong">why sparse matrices ops are slow?</strong></p><p name="7cad" id="7cad" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">If you look at the memory access pattern you need to make a sparse matrix/ matrix multiplication, you’ll see that by definition it’s hard to have those blocks of 16 floats when reading the matrix weights</strong>. And reading 16 contiguous floats is just a minimum, you’ll need to read more data at once to reach full performance.</p><p name="1ba1" id="1ba1" class="graf graf--p graf-after--p">That explains why a naive implementation can be at least an order of magnitude slower than the dense version.</p><p name="0a6e" id="0a6e" class="graf graf--p graf-after--p">Unless you make some compromise and use a <strong class="markup--strong markup--p-strong">block sparse matrix</strong>: each block, if large enough, will produce <strong class="markup--strong markup--p-strong">large contiguous accesses</strong>. 8x8 blocks is a minimum in <a href="https://openai.com/blog/block-sparse-gpu-kernels/" data-href="https://openai.com/blog/block-sparse-gpu-kernels/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">OpenAI implementation</a>, but you will get even better performance with 32x32 blocks.</p><p name="deef" id="deef" class="graf graf--p graf-after--p">But of course, you have to make sure that your model is working in a similar fashion with block sparse compared to pure sparse matrices. It can be the case if your matrices are large enough so block size is small in comparison, but you have to check.</p><p name="76d7" id="76d7" class="graf graf--p graf-after--p">The other way is to convince an executive at Nvidia to add some hardware sparse support into their next-gen GPU, and now it’s done. More on this below!</p><h4 name="6fbe" id="6fbe" class="graf graf--h4 graf-after--p">Inter-GPU memory transfer</h4><p name="e665" id="e665" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Memory bottlenecks exist within the GPU, but if you work with multiple GPUs sharing a single model, the available bandwidth is way lower than between memory and cores.</strong></p><p name="3423" id="3423" class="graf graf--p graf-after--p">The <a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" data-href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">DataParallel</strong></a> node of <strong class="markup--strong markup--p-strong">PyTorch</strong> is convenient, but it is no magic: after each batch, the GPUs must send their gradients to a single GPU, and then this latter must broadcast the updated model to each GPU. <strong class="markup--strong markup--p-strong">If your model is big enough, this transfer can take very significant time, and the performance will suffer. </strong>Another point is that the transfers are synchronous, no GPU can work if the new model has not been received.</p><p name="66c8" id="66c8" class="graf graf--p graf-after--p">Another way to use multiple GPUs is to split a single model between the different GPUs, and then transfer only the “frontier” layers from a GPU to the next. Same thing for backpropagation. This may not be ideal either as the first layer will have to wait for the last to complete before the backpropagation can occur. The performance will depend heavily on the morphology of the network.</p><h3 name="9a53" id="9a53" class="graf graf--h3 graf-after--p">Ampere Highlights</h3><p name="4354" id="4354" class="graf graf--p graf-after--h3">Let’s finish where we started, with the latest Nvidia announcement.</p><h4 name="2767" id="2767" class="graf graf--h4 graf-after--p">Tensor Cores</h4><p name="44ce" id="44ce" class="graf graf--p graf-after--h4">With Volta, Nvidia introduced new “<strong class="markup--strong markup--p-strong">Tensor Core units</strong>”, and it looks like they are here to stay. Turing and now <strong class="markup--strong markup--p-strong">Ampere</strong> iterated on these new units.</p><p name="7bbe" id="7bbe" class="graf graf--p graf-after--p">You can see them as ultra-specialized units, with some significant dedicated silicon.</p><p name="e625" id="e625" class="graf graf--p graf-after--p">And this means a lot in terms of speed, especially quantized networks inference :</p><figure name="dabb" id="dabb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*e7SMvK3WdZmz32poFUQaJA.gif" data-width="1024" data-height="576" src="https://cdn-images-1.medium.com/max/800/1*e7SMvK3WdZmz32poFUQaJA.gif"></div><figcaption class="imageCaption">From <a href="https://youtu.be/yyR0ZoCeBO8?t=19" data-href="https://youtu.be/yyR0ZoCeBO8?t=19" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://youtu.be/yyR0ZoCeBO8?t=19</a></figcaption></figure><p name="a0f7" id="a0f7" class="graf graf--p graf-after--figure">For training, it was a bit more difficult on Volta, as working with FP16 was possible but a bit tricky (the 8x gain in speed was indeed tempting).</p><p name="dbea" id="dbea" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">But now with Ampere, Nvidia announces support for FP32 and even FP64 for Tensor Cores.</strong> <strong class="markup--strong markup--p-strong">And it looks like FP32 is now 20 times faster than on Volta with sparsity, and 10 times without sparsity.</strong> And this is for training and inference because it’s just big tensor ops, nothing special here.</p><p name="1c9d" id="1c9d" class="graf graf--p graf-after--p">It looks like we’ll be getting some nice toys to play with.</p><figure name="60dc" id="60dc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*rBrF53VkNYC5gL6YJExXxQ.png" data-width="1920" data-height="1080" src="https://cdn-images-1.medium.com/max/800/1*rBrF53VkNYC5gL6YJExXxQ.png"></div></figure><h4 name="c0fc" id="c0fc" class="graf graf--h4 graf-after--figure">Sparsity</h4><p name="e2ef" id="e2ef" class="graf graf--p graf-after--h4">From the <a href="https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/#" data-href="https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/#" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Nvidia Blog</a> :</p><blockquote name="eb97" id="eb97" class="graf graf--blockquote graf-after--p">NVIDIA has developed a simple and universal recipe for sparsifying deep neural networks for inference using this 2:4 structured sparsity pattern.</blockquote><figure name="8946" id="8946" class="graf graf--figure graf-after--blockquote"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 393px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.10000000000001%;"></div><img class="graf-image" data-image-id="1*xq_MGzOOGVazlgAT8766uA.png" data-width="1382" data-height="775" src="https://cdn-images-1.medium.com/max/800/1*xq_MGzOOGVazlgAT8766uA.png"></div></figure><p name="6098" id="6098" class="graf graf--p graf-after--figure">If you have read the first part of this series, you should feel at home.</p><p name="6480" id="6480" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The idea</strong> is simple: maybe using a fully dense matrix is not useful. And what Nvidia is claiming is that it’s true, <strong class="markup--strong markup--p-strong">keeping only half the weights has a minimal impact on precision.</strong></p><p name="4ac9" id="4ac9" class="graf graf--p graf-after--p">And so they propose a method to reduce the number of weights. <strong class="markup--strong markup--p-strong">But what is more interesting, is that the A100 GPU has new instructions to process efficiently these sparse matrices, at twice the speed of dense ones </strong>(no magic here, only half the multiply occurs of course).</p><p name="16b7" id="16b7" class="graf graf--p graf-after--p">So anyone can try its own method to sparsify the matrices and use the new instructions to speed things up. <strong class="markup--strong markup--p-strong">The only constraint is that the sparse pattern is fixed, as every 4 cells must have 2 sparse ones at most.</strong></p><p name="007c" id="007c" class="graf graf--p graf-after--p">You can compare this to the way textures are compressed to save memory but for floating computation and not just graphics.</p><p name="ce75" id="ce75" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">I see it mostly for inference at first, but I am sure some clever people will come with imaginative ways to use those new capabilities for training too, as it’s just some new compute ops.</strong></p><p name="cf1e" id="cf1e" class="graf graf--p graf-after--p">What about “sparse block sparse matrices”, by combining soon to be released OpenAI “block sparse matrices” with this? We’ll see.</p><h3 name="6424" id="6424" class="graf graf--h3 graf-after--p">Conclusion</h3><p name="42c5" id="42c5" class="graf graf--p graf-after--h3">I hope you enjoyed this second part of our trip to sparse land, even if it may have been a bit harder to digest.</p><p name="fdb3" id="fdb3" class="graf graf--p graf-after--p">I hope too this will help you to better understand th<strong class="markup--strong markup--p-strong">e level of mastery developers in the PyTorch or Keras team show</strong>: they manage to <strong class="markup--strong markup--p-strong">hide all this complexity </strong>and make it easy for mere mortals to use these supercomputer-on-a-chip to their full power, in just a <strong class="markup--strong markup--p-strong">few lines of python.</strong></p><p name="b4cf" id="b4cf" class="graf graf--p graf-after--p">Next time we will get back to more usual depths: we’ll see some<strong class="markup--strong markup--p-strong"> techniques we can use to train sparse networks</strong>, and how performance is impacted.</p><p name="4bec" id="4bec" class="graf graf--p graf-after--p graf--trailing">By the way, congrats to Victor Sanh, Thomas Wolf, and Alexander M. Rush for their latest paper “<a href="https://arxiv.org/abs/2005.07683" data-href="https://arxiv.org/abs/2005.07683" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Movement Pruning: Adaptive Sparsity by Fine-Tuning</a>”!</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@francois.lagunas" class="p-author h-card">François Lagunas</a> on <a href="https://medium.com/p/b8bc9ce950fc"><time class="dt-published" datetime="2020-05-28T21:23:50.389Z">May 28, 2020</time></a>.</p><p><a href="https://medium.com/@francois.lagunas/sparse-neural-networks-2-n-gpu-performance-b8bc9ce950fc" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on June 12, 2020.</p></footer></article></body></html>