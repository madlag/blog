<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Is the future of Neural Networks Sparse? An Introduction (1/N)</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Is the future of Neural Networks Sparse? An Introduction (1/N)</h1>
</header>
<section data-field="subtitle" class="p-summary">
From principles to real-world library support.
</section>
<section data-field="body" class="e-content">
<section name="1ed2" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="a65d" id="a65d" class="graf graf--h3 graf--leading graf--title">Is the future of Neural Networks Sparse? An Introduction (1/N)</h3><h4 name="6aea" id="6aea" class="graf graf--h4 graf-after--h3 graf--subtitle">From principles to real-world library support.</h4><figure name="e469" id="e469" class="graf graf--figure graf-after--h4"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 291px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.6%;"></div><img class="graf-image" data-image-id="1*7e9p9imPoRBYh5WYF3FFrQ.png" data-width="1034" data-height="430" src="https://cdn-images-1.medium.com/max/800/1*7e9p9imPoRBYh5WYF3FFrQ.png"></div><figcaption class="imageCaption">TLDR: Yes</figcaption></figure><h4 name="d44a" id="d44a" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Hi, I am François Lagunas.</strong></h4><p name="f134" id="f134" class="graf graf--p graf-after--h4">I am doing Machine Learning research, and I have been working for the last months on using sparse matrices, especially in Transformers. The recent <a href="https://openai.com/blog/openai-pytorch/" data-href="https://openai.com/blog/openai-pytorch/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">announcement</strong></a> that <strong class="markup--strong markup--p-strong">OpenAI</strong> is porting its <a href="https://openai.com/blog/block-sparse-gpu-kernels/" data-href="https://openai.com/blog/block-sparse-gpu-kernels/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">block sparse toolbox</strong></a> in <strong class="markup--strong markup--p-strong">PyTorch</strong> is really big news:</p><blockquote name="0932" id="0932" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“We are in the process of writing PyTorch bindings for our highly-optimized blocksparse kernels, and will open-source those bindings in upcoming months”</blockquote><p name="f6e3" id="f6e3" class="graf graf--p graf-after--blockquote">I was talking about it with the outstanding <a href="https://huggingface.co/" data-href="https://huggingface.co/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face</a> team, (I am one of their early investors), and I wanted to share with you my excitement!</p><h3 name="5311" id="5311" class="graf graf--h3 graf-after--p">What is a Sparse Matrix?</h3><p name="34c6" id="34c6" class="graf graf--p graf-after--h3">A <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">sparse</em></strong> matrix is just a matrix with some zeros. Usually, a lot of them. So every place you are using a <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">dense matrix</em></strong>, in a linear layer, for example, you could be using a sparse one.</p><figure name="0d4c" id="0d4c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 201px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 28.7%;"></div><img class="graf-image" data-image-id="1*tf99LCAMrO70WAO4tkgFBw.png" data-width="1680" data-height="482" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*tf99LCAMrO70WAO4tkgFBw.png"></div><figcaption class="imageCaption">Matrices with increasing sparsity</figcaption></figure><p name="0808" id="0808" class="graf graf--p graf-after--figure">The <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">sparsity</em></strong> of the matrix is the fraction of zeros against the size of the matrix</p><p name="2f38" id="2f38" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The pros?</strong> If you have a lot of zeros, you don’t have to compute some multiplications, and you don’t have to store them. So you <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">may</em></strong> gain on size and speed, for training and inference (more on this today).</p><p name="617a" id="617a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The cons? </strong>Of course, having all these zeros will probably have an impact on network accuracy/performance. But to what extent? You may be surprised.</p><h3 name="e387" id="e387" class="graf graf--h3 graf-after--p">Where are they from?</h3><p name="9cd2" id="9cd2" class="graf graf--p graf-after--h3">The first researchers/engineers to use sparse matrices were <a href="https://en.wikipedia.org/wiki/Finite_element_method" data-href="https://en.wikipedia.org/wiki/Finite_element_method" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Finite Elements</a> users.</p><figure name="6b0b" id="6b0b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 332px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.4%;"></div><img class="graf-image" data-image-id="1*IvRIZEjC7kgUBuozUFlSng.png" data-width="760" data-height="360" src="https://cdn-images-1.medium.com/max/800/1*IvRIZEjC7kgUBuozUFlSng.png"></div><figcaption class="imageCaption">A 2D mesh (roof of Omni Coliseum, Atlanta) and its finite element matrix (<a href="https://www.cise.ufl.edu/research/sparse/matrices/HB/bcsstk14.html" data-href="https://www.cise.ufl.edu/research/sparse/matrices/HB/bcsstk14.html" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">source</a>).</figcaption></figure><p name="ced0" id="ced0" class="graf graf--p graf-after--figure">When you have to deal with large physical simulations, you get a large graph of interconnected vertices.</p><p name="3b3c" id="3b3c" class="graf graf--p graf-after--p">Each vertex is a point of your system, and each edge connects two vertices. That means that these <strong class="markup--strong markup--p-strong">two points</strong> will have some <strong class="markup--strong markup--p-strong">influence</strong> on each other in the model. And so there is a <strong class="markup--strong markup--p-strong">non-zero</strong> value in the matrix that describes the graph.</p><p name="9814" id="9814" class="graf graf--p graf-after--p">This last sentence sums it up: you need non-zero values in the matrix when two dimensions are interacting in some way.</p><p name="6299" id="6299" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Now getting back to ML, you should ask yourself the same question: are all the dimensions of my input vector interacting with all the others? </strong>Usually not. So going sparse maybe useful.</p><p name="1db8" id="1db8" class="graf graf--p graf-after--p">We have actually a very good, and famous, example of a successful trip to sparse-land: <strong class="markup--strong markup--p-strong">convolutional layers</strong>.</p><figure name="04aa" id="04aa" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 627px; max-height: 248px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39.6%;"></div><img class="graf-image" data-image-id="1*3WLh11vam1ktq7kWJ9aKpg.jpeg" data-width="627" data-height="248" src="https://cdn-images-1.medium.com/max/800/1*3WLh11vam1ktq7kWJ9aKpg.jpeg"></div><figcaption class="imageCaption">Learned convolutional filters. From <a href="http://cs231n.github.io/convolutional-networks/" data-href="http://cs231n.github.io/convolutional-networks/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">http://cs231n.github.io/convolutional-networks/</a></figcaption></figure><p name="a6a2" id="a6a2" class="graf graf--p graf-after--figure">Convolutional layers are a smart and efficient way to implement a sparse transformation on an input tensor.</p><p name="95d0" id="95d0" class="graf graf--p graf-after--p">When processing images, it comes down to two things:</p><p name="c279" id="c279" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Sparsity</strong>: the transformation is local → each output pixel should depend on a few neighboring input pixels.</p><p name="48ad" id="48ad" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Invariance</strong>: the transformation does not depend on the position in the image</p><p name="a6ca" id="a6ca" class="graf graf--p graf-after--p">Then you just add the constraint that the transformation is linear: if you were to represent this transformation, you would get a HUGE matrix with only a few non-zeros. But of course, the right way to do this is to do a multiplication of the input tensor with a small set of small matrices (each square in the image before).</p><p name="2637" id="2637" class="graf graf--p graf-after--p">The importance of convolutions in today’s ML success is obvious. But you can see that <strong class="markup--strong markup--p-strong">finding a clever way to make things sparse sounds like a good recipe to save time and space.</strong></p><h3 name="04cd" id="04cd" class="graf graf--h3 graf-after--p">Where are they useful?</h3><p name="b337" id="b337" class="graf graf--p graf-after--h3">Convolutions are already an efficient form of sparsity, so you could try to make them <a href="https://arxiv.org/abs/1902.05967" data-href="https://arxiv.org/abs/1902.05967" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">even</a> more <a href="http://arxiv.org/abs/1907.04840" data-href="http://arxiv.org/abs/1907.04840" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">sparse</a>, but some other networks contain much larger matrices that may benefit from sparsity: Transformers.</p><p name="16d2" id="16d2" class="graf graf--p graf-after--p">And those are getting bigger and bigger. We have greatly exceeded the 1 billion parameters in 2019, and it’s not stopping here. The cost to train and to use those networks is getting unpractical, so every method to reduce their size will be welcome.</p><figure name="61ea" id="61ea" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 337px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48.1%;"></div><img class="graf-image" data-image-id="0*m3oTlwLmwuXuBCVg.jpg" data-width="850" data-height="409" src="https://cdn-images-1.medium.com/max/800/0*m3oTlwLmwuXuBCVg.jpg"></div><figcaption class="imageCaption">From <a href="https://devblogs.nvidia.com/training-bert-with-gpus/" data-href="https://devblogs.nvidia.com/training-bert-with-gpus/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://devblogs.nvidia.com/training-bert-with-gpus/</a></figcaption></figure><h3 name="ed0d" id="ed0d" class="graf graf--h3 graf-after--figure">Why the OpenAI announcement is so important?</h3><p name="ed7c" id="ed7c" class="graf graf--p graf-after--h3">So, if everything is fine in sparse-land, we should all be trying sparse matrices, shouldn’t we?</p><p name="155e" id="155e" class="graf graf--p graf-after--p">Yes. But there is this stupid thing called <strong class="markup--strong markup--p-strong">implementation</strong>. It’s easy to see the theoretical improvements we could get with sparse compute. But the support in libraries is quite … sparse.</p><p name="6130" id="6130" class="graf graf--p graf-after--p">PyTorch <a href="https://github.com/soumith" data-href="https://github.com/soumith" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">developers</a>, for example, have done a <strong class="markup--strong markup--p-strong">significant</strong> <strong class="markup--strong markup--p-strong">effort</strong> to support sparse compute. But there is still a big gap in performance between dense and sparse matrices operations, which defeats the whole purpose of using them. Even memory usage is quite large: sparsity has to be more than 80% to save some room on sparse matrices (more on that in my next post). Even basic serialization was broken before version 1.4. The reason is that the underlying libraries (for example cuSPARSE) are not doing a great job because the problem is ill-suited to the way GPU works.</p><p name="711c" id="711c" class="graf graf--p graf-after--p">So the <strong class="markup--strong markup--p-strong">OpenAI</strong> <strong class="markup--strong markup--p-strong">announcement</strong> on their block sparse tools is <strong class="markup--strong markup--p-strong">very</strong> <strong class="markup--strong markup--p-strong">good</strong> <strong class="markup--strong markup--p-strong">news</strong> for those who want to use sparse ops without sacrificing training speed (and it looks like some <a href="https://github.com/openai/blocksparse/issues/2" data-href="https://github.com/openai/blocksparse/issues/2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">people</a> have been waiting for some time now). And we are not talking about a few percents.</p><blockquote name="dc23" id="dc23" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“Our kernels typically performed <strong class="markup--strong markup--blockquote-strong">one or two orders of magnitude faster</strong> in terms of GFLOPS.”</blockquote><figure name="7346" id="7346" class="graf graf--figure graf-after--blockquote"><div class="aspectRatioPlaceholder is-locked" style="max-width: 664px; max-height: 356px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 53.6%;"></div><img class="graf-image" data-image-id="1*qXMoK7emiT7J6CA_3O29_A.png" data-width="664" data-height="356" src="https://cdn-images-1.medium.com/max/800/1*qXMoK7emiT7J6CA_3O29_A.png"></div><figcaption class="imageCaption">From OpenAI <a href="https://d4mucfpksywv.cloudfront.net/blocksparse/blocksparsepaper.pdf" data-href="https://d4mucfpksywv.cloudfront.net/blocksparse/blocksparsepaper.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">blocksparse paper</a></figcaption></figure><p name="ab50" id="ab50" class="graf graf--p graf-after--figure">(The worst thing is that the <a href="https://d4mucfpksywv.cloudfront.net/blocksparse/blocksparsepaper.pdf" data-href="https://d4mucfpksywv.cloudfront.net/blocksparse/blocksparsepaper.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">paper</a> concludes that cuBLAS is faster that cuSPARSE even with very sparse matrices. How sad.)</p><p name="9a9d" id="9a9d" class="graf graf--p graf-after--p">The magic keyword here is “<strong class="markup--strong markup--p-strong">block</strong>”. <strong class="markup--strong markup--p-strong">It’s hard to implement general sparse matrice computations on GPUs in an efficient way</strong>. But it gets much easier if you add a “reasonable” constraint on the form of the matrices: their non-zeros should be grouped in small fixed-size blocks, and that makes GPU processing much easier to parallelize efficiently. Typically 8x8, 16x16 or 32x32 blocks, 16x16 already giving a very good performance, with 32x32 giving a slightly better one.</p><figure name="3a23" id="3a23" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 338px; max-height: 330px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 97.6%;"></div><img class="graf-image" data-image-id="1*rHFMCfJ8Td-vhJi0e0zAQw.png" data-width="338" data-height="330" src="https://cdn-images-1.medium.com/max/800/1*rHFMCfJ8Td-vhJi0e0zAQw.png"></div><figcaption class="imageCaption">A 8-block-sparse matrice</figcaption></figure><p name="e957" id="e957" class="graf graf--p graf-after--figure">Of course, the “block” constraint may be crippling some sparsification algorithms, or at least it would require some changes to take it into account.</p><p name="e3ef" id="e3ef" class="graf graf--p graf-after--p">But at least we can play with large high sparsity matrices, and the block constraint may not be a big issue: if you think about it, it means that there is <strong class="markup--strong markup--p-strong">some locality in the dimensions</strong>, and that sounds a quite reasonable constraint. That’s the same reason band matrices have been useful in the past (finite difference, finite elements), and it was a much stronger constraint.</p><figure name="1f8d" id="1f8d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 338px; max-height: 330px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 97.6%;"></div><img class="graf-image" data-image-id="1*zknKSiBQpsppvjDJFIFbqw.png" data-width="338" data-height="330" src="https://cdn-images-1.medium.com/max/800/1*zknKSiBQpsppvjDJFIFbqw.png"></div><figcaption class="imageCaption">Band matrix</figcaption></figure><h3 name="76bd" id="76bd" class="graf graf--h3 graf-after--figure">Conclusion</h3><p name="8076" id="8076" class="graf graf--p graf-after--h3">I hope I have convinced you that 2020 will be the sparse network year (it already has two zeros, that’s a sign).</p><p name="f253" id="f253" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Next time </strong>for those who are curious about what happens when they are using some CUDA based PyTorch code, we’ll dig a bit deeper in <strong class="markup--strong markup--p-strong">GPU internals</strong>, (and we will understand<strong class="markup--strong markup--p-strong"> why block sparse code is outrunning sparse code by a large margin</strong>).</p><p name="d529" id="d529" class="graf graf--p graf-after--p graf--trailing"><strong class="markup--strong markup--p-strong">This article series will continue on the different techniques that have been proposed to make sparse networks, and what are the potential long term benefits.</strong></p></div></div></section><section name="61f3" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="402a" id="402a" class="graf graf--h4 graf--leading">More reading</h4><p name="da09" id="da09" class="graf graf--p graf-after--h4">First, here is a <a href="https://towardsdatascience.com/sparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71" data-href="https://towardsdatascience.com/sparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">study</strong></a><strong class="markup--strong markup--p-strong"> of PyTorch sparse performance.</strong></p><p name="1813" id="1813" class="graf graf--p graf-after--p">If you want to have a very detailed review of <strong class="markup--strong markup--p-strong">different complementary approaches to network size reduction</strong>, and not just about sparse ones, you should definitely read <a href="http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html" data-href="http://mitchgordon.me/machine/learning/2020/01/13/do-we-really-need-model-compression.html" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">this article</a>.</p><p name="ae92" id="ae92" class="graf graf--p graf-after--p graf--trailing">And if you want to <strong class="markup--strong markup--p-strong">create illustrations like the header of this blog post</strong>, you will find the code I used on my <a href="https://github.com/madlag/medium_posts/tree/master/sparse_matrices_1" data-href="https://github.com/madlag/medium_posts/tree/master/sparse_matrices_1" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">github</strong></a>.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@francois.lagunas" class="p-author h-card">François Lagunas</a> on <a href="https://medium.com/p/d03923ecbd70"><time class="dt-published" datetime="2020-02-04T17:08:58.250Z">February 4, 2020</time></a>.</p><p><a href="https://medium.com/@francois.lagunas/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on June 12, 2020.</p></footer></article></body></html>